---
title: "Quizzes and Problems"
published: true
morea_id: assessment-cp
morea_summary: "Conditional Probabilities and Independence"
morea_outcomes_assessed:
 # - outcome-CHANGE-ME
morea_type: assessment
morea_start_date: "2023-09-10"
morea_labels:
---
# Quiz 3 and 4

Please submit on Laulima as instructed

# Problems in class

The four problems here are a developed from the corresponding
problems in the previous module. 

1. Recall the coin toss problem (Problem 1 from the previous module).

    1. Suppose the coin tosses are fair and every sequence of coin
       tosses has equal probability. What is the probability the
	   first coin is heads given the second and third coins have the
	   same face (ie. the event that they are both heads or both tails)?
	   
	2. Instead, suppose the coin tosses are rigged. The person tossing
       the coin ensures that you only see an even number of Heads, but
       makes sure that all sequences with an even number of Heads have
       equal probabilities (whether 0 or 2 Heads among 3 coin
       tosses). What is the probability the
	   first coin is heads given the second and third coins have the
	   same face (ie. the event that they are both heads or both tails)?
	   
    3. Under each of the probability models, show that every pair of
		coin tosses are independent. In the second model, show that the
		three coin tosses are *not* independent.
		
	4. Can you give another probability assignment on the coin tosses such
		that the event: "first coin shows Heads" is independent of 
		any event you could construct from the second and third coins alone?

2. Let us build on our abstract model of a communication link. As before,
    there is a transmitter and a receiver, and the transmitter
    transmits one bit through a channel. The transmitter chooses 0 or
    1, but unlike the previous module, now the probability of a 1 is
    $$p$$. Now the link is not perfect. Given any input, the channel
    flips the input bit with probability $$\epsilon$$. This abstraction
	is called a _Binary Symmetric Channel_.
	
	  1. Phrase this channel model in terms of conditional probabilities.

	  2. If $$\epsilon = \frac12$$, show that the received bit is independent of the transmitted bit. In this case of course, the implication is that there is nothing we could do to reconstruct the transmitted bit. 
    

3. We set up _QuickSort_ in the last module. Please review the setup.
    You set up the probability space for a sequence of length 4. In the 
	first iteration, you would choose a pivot, say $$p_0$$, and break
	the sequence into two, calling the procedure recursively on each
	subsequence.
	
	  1. Let $$L_1$$ and $$L_2$$ be the lengths of the two sequences when you split the length 4 sequence. What is the distribution of $$L_1$$? Of $$L_2$$? Is it intuitive that they should have distributions related that way?
	   
	  2. This anticipates a future topic: given $$L_1$$, what is the distribution of $$L_2$$? 
	   
	  3. When _Quicksort_ is called on the two subsequences of length $$L_1$$ and $$L_2$$, each call produces a new pivot, say $$p_1$$ and $$p_2$$ respectively. Conditioned on any choice of $$p_0$$, the two pivots are chosen independently. But are $$p_0$$ and $$p_1$$ independent? (You can tell whether they are independent without any calculation).
	   
	  4. This question anticipates a future topic, but what is the _joint_ distribution of $$p_1$$ and $$p_2$$? We have not formally defined the joint distribution yet, but this problem helps you appreciate the need for one.   
  
  
    We are handcalculating everything for now because we want to
  understand what is happening. Consequently, we are keeping the
  sequence length small. You could easily simulate everything we are
  doing here for larger sequence lengths. But simulations like this
  not only tend to be overkill, but also may not reveal much insights,
  because they carry too much detail (like the distribution of $$p_1$$
  and $$p_2$$) that are not really that important. If we are
  interested in running time, we will be able to identify a few
  summaries to keep track of (we will learn them in Expectations of
  Random Variables). We can often hand-calculate or estimate these
  summaries better and faster than any simulation.
		
4. AI and Machine learning approaches almost always have a
    probabilistic setup. We considered a classification problem in the
    prior module.  where a threshold (a number $$T$$) splits the line
    into two (infinite length) intervals, the left side of $$T$$
    carrying label -1 and the right side carrying label +1. The
    threshold $$T$$ is unknown and the learning algorithm is supposed
    to figure this out.
	
	Training points are generated by a distribution $$D$$, which we
	left unspecified in the formulation last module. To generate a
	training point from $$D$$, we first toss a biased coin whose two
	sides are $$-1$$ and $$+1$$, the probability of landing on $$+1$$
	being $$p$$. If the outcome of the coin toss is $$-1$$, we choose
	a number from $$T-1$$ to $$T$$, using a uniform pdf. If the coin
	toss is $$+1$$, we choose a number from $$T$$ to $$T+1$$, using a
	uniform pdf.
	
	Our learning algorithm sees a training point $$x$$ and its label.
	If the label of the training point $$x$$ is -1, the algorithm
	outputs as its estimate of the threshold to be $$x+1-p$$, If the
	label of the training point $$x$$ is +1, the algorithm outputs
	$$x-p$$.
	
	  a. What is the generalization error (the probability another
      point generated independently from $$D$$ is misclassified with
      the estimated threshold) if the label of the training point was
      $$-1$$? What if the label was $$+1$$?
	  
	  b. What is the generalization error? The distinction between this
	  and the prior quesiton motivates the topics in the next module.
  
  




