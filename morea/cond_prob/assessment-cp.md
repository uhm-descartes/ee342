---
title: "Quizzes and Problems"
published: false
morea_id: assessment-cp
morea_summary: "Conditional Probabilities and Independence"
morea_outcomes_assessed:
 # - outcome-CHANGE-ME
morea_type: assessment
morea_start_date: "2023-09-10"
morea_labels:
---
# Quiz 3 and 4

Please submit on Laulima as instructed

# Problems in class

The four problems here are a developed from the corresponding
problems in the previous module. 

1. Recall the coin toss problem (Problem 1 from the previous module).

    b. Suppose the coin tosses are fair and every sequence of coin
       tosses has equal probability. What is the probability the
	   first coin is heads given the second and third coins have the
	   same face (ie. the event that they are both heads or both tails)?
	   
	c. Instead, suppose the coin tosses are rigged. The person tossing
       the coin ensures that you only see an even number of Heads, but
       makes sure that all sequences with an even number of Heads have
       equal probabilities (whether 0 or 2 Heads among 3 coin
       tosses). What is the probability the
	   first coin is heads given the second and third coins have the
	   same face (ie. the event that they are both heads or both tails)?
	   
    d. Under each of the probability models, show that every pair of
		coin tosses are independent. In the second model, show that the
		three coin tosses are *not* independent.
		
	e. Can you give another probability assignment on the coin tosses such
		that the event: "first coin shows Heads" is independent of 
		any event you could construct from the second and third coins alone?

2. Let us build on our abstract model of a communication link. As before,
    there is a transmitter and a receiver, and the transmitter
    transmits one bit through a channel. The transmitter chooses 0 or
    1, but unlike the previous module, now the probability of a 1 is
    $$p$$. Now the link is not perfect. Given any input, the channel
    flips the input bit with probability $$\epsilon$$. This abstraction
	is called a _Binary Symmetric Channel_. 
		a. Phrase this channel model in terms of conditional
    probabilities.
		b. If $$\epsilon = \frac12$$, show that the received bit is independent of the transmitted bit. In this case of course, the implication is that there is nothing we could do to reconstruct the transmitted bit. 
    

3. _QuickSort_ is a recursive, divide-and-conquer algorithm to sort a
  sequence. Some of you may have seen this before. Each call to
  QuickSort works as follows: given $$n$$ numbers to sort (in
  ascending order, say),
	a. pick a _pivot_ at random from one of the $$n$$ elements.

	b. all numbers $$\le$$ the pivot are placed in the left bin, and
       all numbers $$>$$ than the pivot are placed in the right bin
       (so we do $$n$$ comparisons to partition the numbers into left
       and right bins). The two bins may end up being unequal in size.

    Then Quicksort is called on each bin separately till we end up
	with bins of size 1. Because we also know the relative order of
	each bin, writing out all the elements in the order of bins gives
	the sorted sequence. This is an example of a randomized
	algorithm. Note that different sequences and different choices of
	pivots lead to different runtimes. Set up a probability space to
	model one step of Quicksort for a random permutation of a sequence
	of 4 distinct numbers. (You can assume without loss of generality
	that the input is a permutation of 1 through 4, do you see why?)

4. AI and Machine learning approaches almost always have a
    probabilistic setup. We consider what is called a classification
    problem. In the real line, imagine that a threshold (a number
    $$T$$) splits the line into two (infinite length) intervals. Each
    side of $$T$$ carries a different _label_, the left side of $$T$$
    carrying label -1 and the right side carrying label +1. The
    threshold $$T$$ is unknown and the learning algorithm is supposed
	to figure this out.
	
	There is a pdf $$D$$ on real numbers. A training point is a real
    number generated by the distribution $$D$$. For the training
    point, the correct label is observed.
	
    The learning algorithm takes one training point and its label, and
    uses it to output a hypothesis (its best guess of what the unknown
    threshold is).  For this problem, consider the following
    algorithm: If the label of the training point $$x$$ is -1, the
    algorithm outputs as its estimate of the threshold to be
    $$x+\frac12$$, labeling the left interval to be -1 and right to be
    +1.  If the label of the training point $$x$$ is +1, the algorithm
    outputs $$x-\frac12$$, again labeling the left interval to be -1
    and right to be +1.
	
	Model the problem, and identify the event that the algorithm makes
    an error, and identify its probability in terms of $$D$$.  This is
    called the _generalization_ error.
  
  




