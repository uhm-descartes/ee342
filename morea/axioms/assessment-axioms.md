---
title: "Proabability Models"
published: true
morea_id: assessment-axioms
morea_summary: "Quiz+Problems"
morea_outcomes_assessed:
 # - outcome-CHANGE-ME
morea_type: assessment
morea_start_date: "2023-09-03"
morea_labels:
---
# Quiz 2

Please access quiz 2 on Laulima and submit it by the due date listed therein.

# Problems in class
There are four problems here. The first is a toy problem, but useful
nonetheless to fix notations. The remaining problems are from
communications, randomized algorithms and machine learning, and they
will be running problems. We will start off simple, and add more to
each of them as we come across new topics. 

1. The first is a is a sequence of problems:

	a. Consider three coin tosses (imagine each coin shows Heads or
	   Tails). At this stage, can you write the sample space?
	   Probabilities? How many distinct events do you have?
  
    b. Suppose the coin tosses are fair and every sequence of coin
       tosses has equal probability. What is the probability
       assignment for this model? 

	c. Instead, suppose we have a special coin, which we will call the
	   Zero-Parity coin. If you toss the coin thrice, you will only
	   see an even number of Heads among the toss outcomes (you will
	   never see 3 Heads for example). In addition, each sequence with
	   an even number of Heads has equal probability (whether the
	   sequence has 0 or 2 Heads among 3 coin tosses). What is the
	   probabilty assignment for this model?
	   
	d. Under each of the probability models, what is the probability
       we observe 2 Heads?

	   If you are wondering, you could realize such "coins" by three
	   _entangled_ quantum electron-spins (often called entangled
	   quantum bits or qubits).  Entangled qubits are a key ingredient
	   in quantum computation. In a similar fashion, while we talk of
	   coin tosses, we are actually thinking of binary state physical
	   systems that could settle in each state with certain
	   probabilities. All quantum states are inherently probabilistic,
	   and all of quantum mechanics tracks these probabilities. I
	   debated using spins for this example instead of pointless
	   tossing of coins, but let us see what you think.
	   
	   
2. Consider the following abstract model of a perfect communication
  link. There is a transmitter and a receiver, and the transmitter
  transmits one bit through a channel. The transmitter chooses 0 or 1
  with equal probability. The channel faithfully reproduces the input,
  so the receiver sees the same bit transmitted by the channel.  Write
  a sample space and a probability assignment to model this problem.

3. _QuickSort_ is a recursive, divide-and-conquer algorithm to sort a
  sequence. Some of you may have seen this before. Each call to
  QuickSort works as follows: given $$n$$ numbers to sort (in
  ascending order, say),
	a. pick a _pivot_ at random from one of the $$n$$ elements.

	b. all numbers $$\le$$ the pivot are placed in the left bin, and
       all numbers $$>$$ than the pivot are placed in the right bin
       (so we do $$n$$ comparisons to partition the numbers into left
       and right bins). The two bins may end up being unequal in size.

    Then Quicksort is called on each bin separately till we end up
	with bins of size 1. Because we also know the relative order of
	each bin, writing out all the elements in the order of bins gives
	the sorted sequence. This is an example of a randomized
	algorithm. Note that different sequences and different choices of
	pivots lead to different runtimes. Set up a probability space to
	model one step of Quicksort for a random permutation of a sequence
	of 4 distinct numbers. (You can assume without loss of generality
	that the input is a permutation of 1 through 4, do you see why?)

4. AI and Machine learning approaches almost always have a
    probabilistic setup. We consider what is called a classification
    problem. In the real line, imagine that a threshold (a number
    $$T$$) splits the line into two (infinite length) intervals. Each
    side of $$T$$ carries a different _label_, the left side of $$T$$
    carrying label -1 and the right side carrying label +1. The
    threshold $$T$$ is unknown (but fixed) to the _learning algorithm_,
    and the algorithm is supposed to figure this out.
	
	There is a pdf $$f$$ on real numbers. A training point is a real
    number $$x$$ generated by the distribution $$f$$. Once we generate the
    training point, an oracle that knows $$T$$ gives us its correct
    label.
	
    The learning algorithm takes this training point $$x$$ and its
    label, and uses it to output a hypothesis (its best guess of what
    the unknown threshold is).  
	
	For this problem, consider the following algorithm: If the label
    of the training point $$x$$ is -1, the algorithm outputs as its
    estimate of the threshold to be $$x+\frac12$$, labeling the left
    interval to be -1 and right to be +1.  If the label of the
    training point $$x$$ is +1, the algorithm outputs $$x-\frac12$$,
    again labeling the left interval to be -1 and right to be +1.
	
	The idea is that since the algorithm does not know $$T$$, it will
	classify points generated by $$f$$ using its estimate of the
	threshold instead. There may therefore be some intervals on the
	real line where the true label (set by $$T$$) and the estimated
	label (set by the algorithm's estimate of the threshold) differ.
	We say we _misclassify_ points in such an interval.
	
	Model the problem, identify the event that the algorithm
    misclassifies points, and identify the probability of
    misclassification in terms of $$f$$.  The probability of
    misclassification is called the _generalization_ error.
  
  







